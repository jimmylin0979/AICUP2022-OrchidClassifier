{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk7BgrzDeFUz"
      },
      "source": [
        "# **匯入相關套件**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8amrK-6eFU6",
        "outputId": "d44e6caa-22fe-4790-80bf-50d70d8941e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch-ema in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (0.3)\n",
            "Requirement already satisfied: torch in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from torch-ema) (1.11.0)\n",
            "Requirement already satisfied: typing-extensions in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from torch->torch-ema) (4.2.0)\n",
            "Requirement already satisfied: transformers in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (4.18.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: requests in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from transformers) (1.22.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from transformers) (2022.3.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: sacremoses in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: click in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from sacremoses->transformers) (8.1.2)\n",
            "Requirement already satisfied: joblib in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
            "Requirement already satisfied: ptflops in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (0.6.9)\n",
            "Requirement already satisfied: torch in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from ptflops) (1.11.0)\n",
            "Requirement already satisfied: typing-extensions in d:\\anaconda3\\envs\\cvnets\\lib\\site-packages (from torch->ptflops) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-ema\n",
        "!pip install transformers\n",
        "!pip install ptflops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BHK3h5M9eFU8"
      },
      "outputs": [],
      "source": [
        "# Pytorch related\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, ExponentialLR\n",
        "\n",
        "# TorchVision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# 3-rd party lib for pytorch\n",
        "## EMA \n",
        "from torch_ema import ExponentialMovingAverage\n",
        "\n",
        "## Flops Calculation\n",
        "from ptflops import get_model_complexity_info\n",
        "\n",
        "## Hugging face API\n",
        "from transformers import AutoFeatureExtractor, SwinForImageClassification   \n",
        "from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\n",
        "from transformers import ViTFeatureExtractor, ViTModel\n",
        "\n",
        "# Other ML related lib that is helpful\n",
        "## Numpy\n",
        "import numpy as np\n",
        "## sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Utils lib\n",
        "## tqdm\n",
        "from tqdm import tqdm \n",
        "\n",
        "# csv\n",
        "import pandas as pd\n",
        "\n",
        "# OS \n",
        "import os\n",
        "import shutil\n",
        "import argparse\n",
        "\n",
        "# #\n",
        "# import models\n",
        "# from data.dataset import OrchidDataSet\n",
        "# from config import DefualtConfig\n",
        "# from utils import get_confidence_score\n",
        "# from utils import mixup_data, mixup_criterion\n",
        "# from utils import rand_bbox\n",
        "# from utils.self_supervised import get_pseudo_labels\n",
        "# from optim.scheduler import GradualWarmupScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80140xpKfRx3",
        "outputId": "4d4bb5f8-f884-4a43-f152-6183139985dd"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# import os\n",
        "# os.chdir('/content/drive/My Drive/AICUP2022 - OrchidClassifier') #切換該目錄\n",
        "# os.listdir() #確認目錄內容"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFSPojDueFU-"
      },
      "source": [
        "# **Configuration 設置擋**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZnCKtPIceFU_"
      },
      "outputs": [],
      "source": [
        "class DefualtConfig(object):\n",
        "\n",
        "    ###################################################################\n",
        "    # Model\n",
        "    # model_name = 'STN_ViT'\n",
        "    # pretrained_model = 'google/vit-base-patch16-224-in21k'\n",
        "    \n",
        "    # model_name = 'ConvNeXt'\n",
        "    # # pretrained_model = 'facebook/convnext-base-224'\n",
        "    # pretrained_model = 'facebook/convnext-base-384'\n",
        "\n",
        "    model_name = 'Swin_ViT'\n",
        "    # pretrained_model = 'microsoft/swin-base-patch4-window7-224'\n",
        "    pretrained_model = 'microsoft/swin-base-patch4-window12-384'\n",
        "\n",
        "    # \n",
        "    resize = 384\n",
        "\n",
        "    ###################################################################\n",
        "\n",
        "    model_path = 'model.pth'\n",
        "    ema_path = 'ema.pth'\n",
        "    load_model = False\n",
        "    num_classes = 219\n",
        "    \n",
        "    ###################################################################\n",
        "    # Mix-based Augmentation\n",
        "    ## Mixup\n",
        "    do_MixUp = True\n",
        "\n",
        "    ## CutMix\n",
        "    do_cutMix = True\n",
        "    beta = 1.0\n",
        "    \n",
        "    # the probability to use mix-based augmentation\n",
        "    mix_prob = 0.2\n",
        "\n",
        "    ###################################################################\n",
        "    # Training\n",
        "    ## Epochs\n",
        "    start_epoch = 0\n",
        "    num_epochs = 105            # Total epoch\n",
        "    earlyStop_interval = 600    # \n",
        "    batch_size = 8\n",
        "    lr = 5e-5\n",
        "    lr_warmup_epoch = 5         # warmup epoch\n",
        "    cosine_tmax = 101           # The Tmax for cosine annealing learning rate scheduler\n",
        "    \n",
        "    # Do semi-supervised training (if there is additional data for training)\n",
        "    do_semi = False\n",
        "    semi_start_epoch = 40\n",
        "\n",
        "    ###################################################################\n",
        "    # GPU Settings\n",
        "    ## The index of GPU to use\n",
        "    use_gpu_index = 0\n",
        "\n",
        "    ###################################################################\n",
        "    # Data\n",
        "    ## DataLoader\n",
        "    num_workers = 6\n",
        "\n",
        "    ## Dataset Location\n",
        "    trainset_path = './data/dataset/train'\n",
        "    unlabeledset_path = './data/dataset/unlabeled'\n",
        "    testset_path = './data/dataset/test'\n",
        "\n",
        "    ## Train/ valid split ratio\n",
        "    train_valid_split = 0.2  # ratio of valid set\n",
        "\n",
        "    ###################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DRJ3RT3eFVB"
      },
      "source": [
        "# **Date processing 資料處理**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCf0Y6C_eFVC"
      },
      "source": [
        "# **Dataset Declaration 資料集宣告**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Jw8Hil-XeFVC"
      },
      "outputs": [],
      "source": [
        "class OrchidDataSet(ImageFolder):\n",
        "\n",
        "    def __init__(self, root, transform_set):\n",
        "\n",
        "        #\n",
        "        super(OrchidDataSet, self).__init__(\n",
        "            root=root, transform=transform_set)\n",
        "\n",
        "    # help to get images for visualizing\n",
        "    def getbatch(self, indices):\n",
        "        '''\n",
        "            @ Params : \n",
        "                1. indices (python.list)\n",
        "            @ Returns : \n",
        "                1. images (torch.tensor with shape (1, ))\n",
        "                2. labels (torch.tensor with shape (1, ))\n",
        "        '''\n",
        "        images = []\n",
        "        labels = []\n",
        "        for index in indices:\n",
        "            image, label = self.__getitem__(index)\n",
        "            # transform_ToTensor =  transforms.Compose([\n",
        "            #                         transforms.Resize((224, 224)),\n",
        "            #                         transforms.ToTensor()])\n",
        "            # image = transform_ToTensor(image)\n",
        "\n",
        "            images.append(image)\n",
        "            labels.append(label)\n",
        "        return torch.stack(images), torch.tensor(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oC4L3LxmepO8"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class TensorIntDataset(Dataset):\n",
        "    ''' Dataset for loading and preprocessing the COVID19 dataset '''\n",
        "\n",
        "    def __init__(self, x, y):\n",
        "        # [x: numpy array, y: list of int]\n",
        "\n",
        "        # convert into Pytorch.torch.tensor\n",
        "        self.data = x\n",
        "\n",
        "        # should be list of int\n",
        "        self.target = y\n",
        "\n",
        "        self.dim = self.data.shape[0]\n",
        "\n",
        "        print('Finished reading TensorInt Dataset ({} samples found)'\n",
        "              .format(len(self.data)))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Returns one sample at a time\n",
        "        return self.data[index], self.target[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the size of the dataset\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHCIgVYpeFVE"
      },
      "source": [
        "# **Model Declaration 模型宣告**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "072c7UgVeFVF"
      },
      "source": [
        "## **Swin Vit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3CcodQ9GeFVF"
      },
      "outputs": [],
      "source": [
        "class Swin_ViT(nn.Module):\n",
        "\n",
        "    def __init__(self, config: DefualtConfig):\n",
        "\n",
        "        super(Swin_ViT, self).__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.num_labels = config.num_classes\n",
        "\n",
        "        ###############################################\n",
        "        # ViT\n",
        "        # pretrained_model = 'microsoft/swin-base-patch4-window7-224'\n",
        "        pretrained_model = config.pretrained_model\n",
        "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(pretrained_model)\n",
        "        self.model = SwinForImageClassification.from_pretrained(pretrained_model)\n",
        "        # print(self.model.config)\n",
        "\n",
        "        # Classifier\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        # self.classifier = nn.Linear(self.model.config.hidden_size, self.num_labels)\n",
        "        self.classifier = nn.Linear(1000, self.num_labels)\n",
        "\n",
        "    def feature_extract(self, imgs):\n",
        "        '''\n",
        "        '''\n",
        "        # Change input array into list with each batch being one element\n",
        "\n",
        "        # convert it to numpy array first\n",
        "        device = torch.device('cpu')\n",
        "        if imgs.device != torch.device('cpu'):\n",
        "            device = torch.device(f'cuda:{self.config.use_gpu_index}')\n",
        "\n",
        "        imgs = imgs.cpu().numpy()\n",
        "        imgs = np.split(np.squeeze(np.array(imgs)), imgs.shape[0])\n",
        "\n",
        "        # Remove unecessary dimension\n",
        "        for index, array in enumerate(imgs):\n",
        "            imgs[index] = np.squeeze(array)\n",
        "\n",
        "        # Apply feature extractor, stack back into 1 tensor and then convert to tensor\n",
        "        # imgs = (batch_size, 3, 224, 224)\n",
        "        imgs = torch.tensor(\n",
        "            np.stack(self.feature_extractor(imgs)['pixel_values'], axis=0))\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        return imgs\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        '''\n",
        "        Model forward function\n",
        "        '''\n",
        "\n",
        "        # Feature extraction\n",
        "        # x = self.feature_extractor(x, return_tensors=\"pt\")\n",
        "        x = self.feature_extract(x)\n",
        "\n",
        "        # Swin-ViT\n",
        "        x = self.model(pixel_values=x)\n",
        "        logits = self.classifier(x.logits)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m59j5FbheFVG"
      },
      "source": [
        "## **ConvNext**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uXVIxe2TeFVH"
      },
      "outputs": [],
      "source": [
        "class ConvNeXt(nn.Module):\n",
        "\n",
        "    def __init__(self, config: DefualtConfig):\n",
        "\n",
        "        super(ConvNeXt, self).__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.num_labels = config.num_classes\n",
        "\n",
        "        ###############################################\n",
        "        # Model\n",
        "        pretrained_model = config.pretrained_model\n",
        "        self.feature_extractor = ConvNextFeatureExtractor.from_pretrained(pretrained_model)\n",
        "        self.model = ConvNextForImageClassification.from_pretrained(pretrained_model)\n",
        "\n",
        "        # Classifier\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(1000, self.num_labels)\n",
        "\n",
        "    def feature_extract(self, imgs):\n",
        "        '''\n",
        "        '''\n",
        "        # Change input array into list with each batch being one element\n",
        "\n",
        "        # convert it to numpy array first\n",
        "        device = torch.device('cpu')\n",
        "        if imgs.device != torch.device('cpu'):\n",
        "            device = torch.device(f'cuda:{self.config.use_gpu_index}')\n",
        "\n",
        "        imgs = imgs.cpu().numpy()\n",
        "        imgs = np.split(np.squeeze(np.array(imgs)), imgs.shape[0])\n",
        "\n",
        "        # Remove unecessary dimension\n",
        "        for index, array in enumerate(imgs):\n",
        "            imgs[index] = np.squeeze(array)\n",
        "\n",
        "        # Apply feature extractor, stack back into 1 tensor and then convert to tensor\n",
        "        # imgs = (batch_size, 3, 224, 224)\n",
        "        imgs = torch.tensor(np.stack(self.feature_extractor(imgs)['pixel_values'], axis=0))\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        return imgs\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        '''\n",
        "        Model forward function\n",
        "        '''\n",
        "\n",
        "        # Feature extraction\n",
        "        x = self.feature_extract(x)\n",
        "\n",
        "        # Swin-ViT\n",
        "        x = self.model(pixel_values=x)\n",
        "\n",
        "        # x = self.dropout(x)\n",
        "        logits = self.classifier(x.logits)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXVZf0jleFVI"
      },
      "source": [
        "## **STN-ViT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JpIGO4doeFVJ"
      },
      "outputs": [],
      "source": [
        "class STN_ViT(nn.Module):\n",
        "\n",
        "    def __init__(self, config: DefualtConfig):\n",
        "\n",
        "        super(STN_ViT, self).__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.num_labels = config.num_classes\n",
        "\n",
        "        ###############################################\n",
        "        # input image with shape (batch_size, 3, 224, 224)\n",
        "        # Spatial transformer localization-network\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=7),\n",
        "            nn.Conv2d(32, 32, kernel_size=7),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Conv2d(32, 32, kernel_size=7),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Conv2d(32, 32, kernel_size=5),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Conv2d(32, 32, kernel_size=5),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # Regressor for the 3 * 2 affine matrix\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(32 * 9 * 9, 90),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(90, 3 * 2)\n",
        "        )\n",
        "        # Initialize the weights/bias with identity transformation\n",
        "        self.fc_loc[-1].weight.data.zero_()\n",
        "        self.fc_loc[-1].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "        ###############################################\n",
        "        # ViT\n",
        "        # pretrained_model = 'google/vit-base-patch16-224-in21k'\n",
        "        pretrained_model = config.pretrained_model\n",
        "        self.feature_extractor = ViTFeatureExtractor.from_pretrained(pretrained_model)\n",
        "        self.vit = ViTModel.from_pretrained(pretrained_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.vit.config.hidden_size, self.num_labels)\n",
        "\n",
        "    def feature_extract(self, imgs):\n",
        "        '''\n",
        "        '''\n",
        "        # Change input array into list with each batch being one element\n",
        "\n",
        "        # convert it to numpy array first\n",
        "        device = torch.device('cpu')\n",
        "        if imgs.device != torch.device('cpu'):\n",
        "            device = torch.device(f'cuda:{self.config.use_gpu_index}')\n",
        "\n",
        "        imgs = imgs.cpu().numpy()\n",
        "        imgs = np.split(np.squeeze(np.array(imgs)), imgs.shape[0])\n",
        "\n",
        "        # Remove unecessary dimension\n",
        "        for index, array in enumerate(imgs):\n",
        "            imgs[index] = np.squeeze(array)\n",
        "\n",
        "        # Apply feature extractor, stack back into 1 tensor and then convert to tensor\n",
        "        # imgs = (batch_size, 3, 224, 224)\n",
        "        imgs = torch.tensor(\n",
        "            np.stack(self.feature_extractor(imgs)['pixel_values'], axis=0))\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        return imgs\n",
        "\n",
        "    def stn(self, x):\n",
        "        '''\n",
        "        Spatial transformer network forward function\n",
        "        '''\n",
        "        xs = self.localization(x)\n",
        "        xs = torch.reshape(xs, (-1, 32 * 9 * 9))\n",
        "        theta = self.fc_loc(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        x = F.grid_sample(x, grid)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        '''\n",
        "        Model forward function\n",
        "        '''\n",
        "\n",
        "        # Feature extraction\n",
        "        x = self.feature_extract(x)\n",
        "\n",
        "        # Spatial Transformer\n",
        "        x = self.stn(x)\n",
        "\n",
        "        # ViT\n",
        "        x = self.vit(pixel_values=x)\n",
        "        # x = torch.mean(x.last_hidden_state[:, ], 1)\n",
        "        x = self.dropout(x.last_hidden_state[:, 0])\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWTYGsubeFVK"
      },
      "source": [
        "# **Learning rate scheduler 學習率調整**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1o-Ts77QeFVK"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "class GradualWarmupScheduler(_LRScheduler):\n",
        "    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n",
        "    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n",
        "    Args:\n",
        "        optimizer (Optimizer): Wrapped optimizer.\n",
        "        multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.\n",
        "        total_epoch: target learning rate is reached at total_epoch, gradually\n",
        "        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
        "        self.multiplier = multiplier\n",
        "        if self.multiplier < 1.:\n",
        "            raise ValueError(\n",
        "                'multiplier should be greater thant or equal to 1.')\n",
        "        self.total_epoch = total_epoch\n",
        "        self.after_scheduler = after_scheduler\n",
        "        self.finished = False\n",
        "        super(GradualWarmupScheduler, self).__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.last_epoch > self.total_epoch:\n",
        "            if self.after_scheduler:\n",
        "                if not self.finished:\n",
        "                    self.after_scheduler.base_lrs = [\n",
        "                        base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "                    self.finished = True\n",
        "                return self.after_scheduler.get_last_lr()\n",
        "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "\n",
        "        if self.multiplier == 1.0:\n",
        "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
        "\n",
        "    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "        # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning\n",
        "        self.last_epoch = epoch if epoch != 0 else 1\n",
        "        if self.last_epoch <= self.total_epoch:\n",
        "            warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch /\n",
        "                                    self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
        "            for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n",
        "                param_group['lr'] = lr\n",
        "        else:\n",
        "            if epoch is None:\n",
        "                self.after_scheduler.step(metrics, None)\n",
        "            else:\n",
        "                self.after_scheduler.step(metrics, epoch - self.total_epoch)\n",
        "\n",
        "    def step(self, epoch=None, metrics=None):\n",
        "        if type(self.after_scheduler) != ReduceLROnPlateau:\n",
        "            if self.finished and self.after_scheduler:\n",
        "                if epoch is None:\n",
        "                    self.after_scheduler.step(None)\n",
        "                else:\n",
        "                    self.after_scheduler.step(epoch - self.total_epoch)\n",
        "                self._last_lr = self.after_scheduler.get_last_lr()\n",
        "            else:\n",
        "                return super(GradualWarmupScheduler, self).step(epoch)\n",
        "        else:\n",
        "            self.step_ReduceLROnPlateau(metrics, epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL4uG1dPeFVL"
      },
      "source": [
        "# **Utils**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg35Zu3veFVM"
      },
      "source": [
        "## **Semi-supervised Learning 自監督式學習**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "r2_sZT2UeFVM"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "###################################################################################\n",
        "\n",
        "config = DefualtConfig()\n",
        "\n",
        "\n",
        "def get_pseudo_labels(model, *datasets, threshold=0.75):\n",
        "    # This functions generates pseudo-labels of a dataset using given model.\n",
        "    # It returns an instance of DatasetFolder containing images whose prediction confidences exceed a given threshold.\n",
        "    # You are NOT allowed to use any models trained on external data for pseudo-labeling.\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # # it costs too more time, uncomment here if you have enough time or have a string GPU\n",
        "    # # combines unlabeled_set & test_sets\n",
        "    # unlabeled, test = datasets\n",
        "    # dataset = ConcatDataset([unlabeled, test])\n",
        "    dataset = datasets[0]\n",
        "\n",
        "    # Construct a data loader.\n",
        "    data_loader = DataLoader(\n",
        "        dataset, batch_size=config.batch_size, shuffle=False)\n",
        "\n",
        "    # Make sure the model is in eval mode.\n",
        "    model.eval()\n",
        "    # Define softmax function.\n",
        "    softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    # temporary variables\n",
        "    maxConfidence, pseudo_label = None, None\n",
        "    masks, img = None, None\n",
        "\n",
        "    # input to dataloader\n",
        "    to_train_x, to_train_y = torch.tensor([]), []\n",
        "    cnt = 0\n",
        "\n",
        "    # Iterate over the dataset by batches.\n",
        "    for batch in tqdm(data_loader):\n",
        "        img, _ = batch\n",
        "\n",
        "        # Forward the data\n",
        "        # Using torch.no_grad() accelerates the forward process.\n",
        "        with torch.no_grad():\n",
        "            logits = model(img.to(device))\n",
        "\n",
        "        # Obtain the probability distributions by applying softmax on logits.\n",
        "        # dimension [128, 11]\n",
        "        probs = softmax(logits)\n",
        "        # probs = probs.cpu().detach().numpy()\n",
        "\n",
        "        # ---------- TODO ----------\n",
        "        # Filter the data and construct a new dataset.\n",
        "\n",
        "        maxConfidence, pseudo_label = torch.max(probs, 1)\n",
        "\n",
        "        # create a mask & delete non-fit datas\n",
        "        masks = (maxConfidence >= threshold)\n",
        "        img = img[masks]\n",
        "        pseudo_label = pseudo_label[masks]\n",
        "\n",
        "        # append result of each batch to all\n",
        "        to_train_x = torch.cat((to_train_x, img), 0)\n",
        "        for label in (pseudo_label):\n",
        "            to_train_y.append(int(label.item()))\n",
        "\n",
        "    cnt = len(to_train_y)\n",
        "\n",
        "    # # Turn off the eval mode.\n",
        "    model.train()\n",
        "\n",
        "    #\n",
        "    if cnt != 0:\n",
        "        print(f\"[ {cnt} Unlabeled Images append into train_set ]\")\n",
        "\n",
        "        # to_train_x = torch.tensor(to_train_x)\n",
        "        # to_train_y = torch.tensor(to_train_y, dtype=torch.int)\n",
        "\n",
        "        # # reshape\n",
        "        to_train_x = torch.reshape(to_train_x, (-1, 3, 224, 224))\n",
        "        # to_train_y = torch.reshape(to_train_y, (-1,))\n",
        "\n",
        "        print(to_train_x.shape)\n",
        "        print(len(to_train_y))\n",
        "\n",
        "        # transfer list of Tensor into dataSet (TensorDataset)\n",
        "        # res_dataset = TensorDataset(to_train_x, to_train_y)\n",
        "        res_dataset = TensorIntDataset(to_train_x, to_train_y)\n",
        "\n",
        "        # free the resources, or it will collapse eventually\n",
        "        del maxConfidence, pseudo_label, masks, img\n",
        "        del to_train_x, to_train_y\n",
        "        gc.collect()\n",
        "\n",
        "        return res_dataset\n",
        "\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TXw3CubeFVN"
      },
      "source": [
        "## **Mix-based Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6FucBmEWeFVN"
      },
      "outputs": [],
      "source": [
        "##########################################################################################\n",
        "# mixup\n",
        "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "##########################################################################################\n",
        "# cutmix\n",
        "def cutmix(batch, alpha):\n",
        "    data, targets = batch\n",
        "\n",
        "    indices = torch.randperm(data.size(0))\n",
        "    shuffled_data = data[indices]\n",
        "    shuffled_targets = targets[indices]\n",
        "\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "\n",
        "    image_h, image_w = data.shape[2:]\n",
        "    cx = np.random.uniform(0, image_w)\n",
        "    cy = np.random.uniform(0, image_h)\n",
        "    w = image_w * np.sqrt(1 - lam)\n",
        "    h = image_h * np.sqrt(1 - lam)\n",
        "    x0 = int(np.round(max(cx - w / 2, 0)))\n",
        "    x1 = int(np.round(min(cx + w / 2, image_w)))\n",
        "    y0 = int(np.round(max(cy - h / 2, 0)))\n",
        "    y1 = int(np.round(min(cy + h / 2, image_h)))\n",
        "\n",
        "    data[:, :, y0:y1, x0:x1] = shuffled_data[:, :, y0:y1, x0:x1]\n",
        "    targets = (targets, shuffled_targets, lam)\n",
        "\n",
        "    return data, targets\n",
        "\n",
        "class CutMixCollator:\n",
        "    def __init__(self, alpha):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        batch = torch.utils.data.dataloader.default_collate(batch)\n",
        "        batch = cutmix(batch, self.alpha)\n",
        "        return batch\n",
        "\n",
        "\n",
        "class CutMixCriterion:\n",
        "    def __init__(self, reduction, label_smoothing=0.1):\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction=reduction, label_smoothing=0.1)\n",
        "\n",
        "    def __call__(self, preds, targets):\n",
        "        targets1, targets2, lam = targets\n",
        "        return lam * self.criterion(\n",
        "            preds, targets1) + (1 - lam) * self.criterion(preds, targets2)\n",
        "\n",
        "##########################################################################################\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = np.int(W * cut_rat)\n",
        "    cut_h = np.int(H * cut_rat)\n",
        "\n",
        "    # uniform\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "    \n",
        "##########################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZpqXeGxeFVO"
      },
      "source": [
        "## **Confidence 模型預測信心水準**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uZrCznHyeFVP"
      },
      "outputs": [],
      "source": [
        "def get_confidence_score(model, loader, topN=5, use_gpu_index='-1', batch_size=32, outpu_file_path='./prediction-Confidence.csv'):\n",
        "    '''\n",
        "    '''\n",
        "\n",
        "    # Set the model state to 'eval', we should not update any parameter here\n",
        "    model.eval()\n",
        "\n",
        "    device = torch.device(f'cuda:{use_gpu_index}' if torch.cuda.is_available(\n",
        "    ) else'cpu') if use_gpu_index != -1 else torch.device('cpu')\n",
        "\n",
        "    #\n",
        "    with torch.no_grad():\n",
        "\n",
        "        with open(outpu_file_path, 'w') as file:\n",
        "\n",
        "            file.write(\n",
        "                f'validset_index, topN, Ground_truth, 1, prob1, 2, prob2, 3, prob3, 4, prob4, 5, prob5\\n')\n",
        "\n",
        "            for batch_idx, batch in enumerate(loader):\n",
        "\n",
        "                # A batch consists of image data and corresponding labels.\n",
        "                imgs, labels = batch\n",
        "\n",
        "                # We don't need gradient in validation.\n",
        "                # Using torch.no_grad() accelerates the forward process.\n",
        "                with torch.no_grad():\n",
        "                    logits = model(imgs.to(device))\n",
        "\n",
        "                y_label = logits.argmax(dim=-1)\n",
        "\n",
        "                # List of\n",
        "                y_probs = [F.softmax(el, dim=0) for i, el in zip(y_label, logits)]\n",
        "\n",
        "                # record the prediction & ground truth for later review\n",
        "                for i, _ in enumerate(y_label):\n",
        "\n",
        "                    img_idx = (batch_idx) * batch_size + i\n",
        "                    # img = ds_valid.__getitem__((batch_idx- 1) * config['batch_size'] + i)\n",
        "                    # # img = np.array(img)\n",
        "\n",
        "                    topN = 5\n",
        "                    # topN_labels = y_probs[i].argsort()[-topN:].tolist()[::-1]\n",
        "                    topN_values, topN_labels = y_probs[i].topk(topN)\n",
        "\n",
        "                    #\n",
        "                    file.write(f'{img_idx}, {topN}, {labels[i].item()}')\n",
        "                    for i in range(topN):\n",
        "                        file.write(\n",
        "                            f', {topN_labels[i].item()}, {topN_values[i].item()}')\n",
        "                    else:\n",
        "                        file.write(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5vCDkdjeFVP"
      },
      "source": [
        "# **訓練**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZXnT3IGeFVP"
      },
      "source": [
        "## **建立 Dataset, Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hcSHLAZLeFVQ"
      },
      "outputs": [],
      "source": [
        "def get_train_valid_ds(ds):\n",
        "\n",
        "    # Split the train/test with each class should appear on both train/test dataset\n",
        "    valid_split = config.train_valid_split\n",
        "\n",
        "    indices = list(range(len(ds)))  # indices of the dataset\n",
        "    train_indices, valid_indices = train_test_split(indices, test_size=valid_split, stratify=ds.targets, random_state=42)\n",
        "    \n",
        "    # Creating sub dataset from valid indices\n",
        "    # Do not shuffle valid dataset, let the image in order\n",
        "    valid_indices.sort()\n",
        "    ds_valid = torch.utils.data.Subset(ds, valid_indices)\n",
        "\n",
        "    ds_train = torch.utils.data.Subset(ds, train_indices)\n",
        "\n",
        "    return ds_train, ds_valid\n",
        "\n",
        "def get_loader(ds):\n",
        "\n",
        "    # Split the train/test with each class should appear on both train/test dataset\n",
        "    valid_split = config.train_valid_split\n",
        "\n",
        "    indices = list(range(len(ds)))  # indices of the dataset\n",
        "    train_indices, valid_indices = train_test_split(\n",
        "        indices, test_size=valid_split, stratify=ds.targets)\n",
        "    \n",
        "    # Creating sub dataset from valid indices\n",
        "    # Do not shuffle valid dataset, let the image in order\n",
        "    valid_indices.sort()\n",
        "    ds_valid = torch.utils.data.Subset(ds, valid_indices)\n",
        "\n",
        "    # Creating PT data samplers and loaders:\n",
        "    train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
        "\n",
        "    # Construct data loaders.\n",
        "    train_loader = DataLoader(\n",
        "        ds, batch_size=config.batch_size, sampler=train_sampler, num_workers=config.num_workers, pin_memory=True)\n",
        "    valid_loader = DataLoader(\n",
        "        ds_valid, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers, pin_memory=True)\n",
        "\n",
        "    return train_loader, valid_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb6u8bOaeFVQ"
      },
      "source": [
        "## **宣告訓練/驗證流程**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G_a9jv0aeFVQ"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, ema):\n",
        "    \n",
        "    # ---------- Training ----------\n",
        "    # Make sure the model is in train mode before training.\n",
        "    model.train()\n",
        "\n",
        "    losses, accs = [], []\n",
        "\n",
        "    # Iterate the training set by batches.\n",
        "    for batch in tqdm(train_loader):\n",
        "\n",
        "        # A batch consists of image data and corresponding labels.\n",
        "        # imgs = (batch_size, 3, 224, 224)\n",
        "        # labels = (batch_size)\n",
        "        imgs, labels = batch\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        target_a, target_b, lam = None, None, None\n",
        "        # do_mix = False\n",
        "        \n",
        "        r = np.random.rand(1)\n",
        "        do_mix = True if r < config.mix_prob else False\n",
        "\n",
        "        r_mix_method = np.random.rand(1)\n",
        "        mix_method = 'cutmix'\n",
        "        if config.do_cutMix and config.do_MixUp:\n",
        "            mix_method = 'cutmix' if r_mix_method < 0.5 else 'mixup'\n",
        "        elif config.do_cutMix:\n",
        "            mix_method = 'cutmix'\n",
        "        elif config.do_MixUp:\n",
        "            mix_method = 'mixup'\n",
        "\n",
        "        if config.do_cutMix and mix_method == 'cutmix':\n",
        "            if config.beta > 0 and do_mix:\n",
        "                # generate mixed sample\n",
        "                do_mix = True\n",
        "                lam = np.random.beta(config.beta, config.beta)\n",
        "                rand_index = torch.randperm(imgs.size()[0]).cuda()\n",
        "                target_a = labels\n",
        "                target_b = labels[rand_index]\n",
        "                bbx1, bby1, bbx2, bby2 = rand_bbox(imgs.size(), lam)\n",
        "                imgs[:, :, bbx1:bbx2, bby1:bby2] = imgs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "                # adjust lambda to exactly match pixel ratio\n",
        "                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (imgs.size()[-1] * imgs.size()[-2]))\n",
        "        else:\n",
        "            labels = labels.to(device)\n",
        "\n",
        "        if config.do_MixUp and do_mix and mix_method == 'mixup':\n",
        "            labels = labels.to(device)\n",
        "            imgs, targets_a, targets_b, lam = mixup_data(imgs, labels, alpha=0.2, use_cuda=torch.cuda.is_available())\n",
        "            imgs, targets_a, targets_b = map(Variable, (imgs, targets_a, targets_b))\n",
        "\n",
        "        # Forward the data. (Make sure data and model are on the same device.)\n",
        "        logits = model(imgs.to(device))\n",
        "\n",
        "        # Calculate the cross-entropy loss.\n",
        "        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n",
        "        # loss = criterion(logits, labels.to(device))\n",
        "\n",
        "        loss = None\n",
        "        if (config.do_cutMix or config.do_MixUp) and do_mix:\n",
        "            if config.do_MixUp and mix_method == 'mixup':\n",
        "                loss = mixup_criterion(criterion, logits, targets_a, targets_b, lam)\n",
        "            elif config.do_cutMix and mix_method == 'cutmix':\n",
        "                # loss = criterion(logits, labels)\n",
        "                target_a = target_a.to(device)\n",
        "                target_b = target_b.to(device)\n",
        "                # lam = lam.to(device)\n",
        "                loss = criterion(logits, target_a) * lam + criterion(logits, target_b) * (1. - lam)\n",
        "        else:\n",
        "            loss = criterion(logits, labels.to(device))\n",
        "\n",
        "        # Gradients stored in the parameters in the previous step should be cleared out first.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute the gradients for parameters.\n",
        "        loss.backward()\n",
        "\n",
        "        # # STN : Allow transformes to do like translation, cropping, isotropic scaling but rotation\n",
        "        # #           , with a intention to let STN learns where to focus on instead of how to transform the image.\n",
        "        # # Below is what matrix should look like : \n",
        "        # #    [ x_ratio, 0 ] [offset_X]\n",
        "        # #    [ 0, y_ratio ] [offset_y]\n",
        "        # if config.model_name == \"STN_ViT\":\n",
        "        #     model.fc_loc[-1].weight.grad[1].zero_()\n",
        "        #     model.fc_loc[-1].weight.grad[3].zero_()\n",
        "\n",
        "        # Update the parameters with computed gradients.\n",
        "        optimizer.step()\n",
        "        ema.update()\n",
        "\n",
        "        # # Clip the gradient norms for stable training.\n",
        "        # grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
        "\n",
        "        # Compute the accuracy for current batch.\n",
        "        # acc = torch.tensor([0])\n",
        "        # if not config.do_cutMix:        \n",
        "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "        accs.append(acc.item())\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return np.mean(accs), np.mean(losses)\n",
        "\n",
        "def valid(model, valid_loader, criterion, ema=None):\n",
        "    # ---------- Validation ----------\n",
        "    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
        "    model.eval()\n",
        "\n",
        "    accs, losses = [], []\n",
        "\n",
        "    if ema is not None:\n",
        "\n",
        "        with ema.average_parameters():\n",
        "\n",
        "            # Iterate the validation set by batches.\n",
        "            for batch in tqdm(valid_loader):\n",
        "\n",
        "                # A batch consists of image data and corresponding labels.\n",
        "                imgs, labels = batch\n",
        "\n",
        "                # We don't need gradient in validation.\n",
        "                # Using torch.no_grad() accelerates the forward process.\n",
        "                with torch.no_grad():\n",
        "                    logits = model(imgs.to(device))\n",
        "\n",
        "                # We can still compute the loss (but not the gradient).\n",
        "                losses.append(criterion(logits, labels.to(device)).item())\n",
        "\n",
        "                # Compute the accuracy for current batch.\n",
        "                accs.append((logits.argmax(dim=-1) == labels.to(device)).float().mean().item())\n",
        "    \n",
        "    else:\n",
        "        \n",
        "        # Iterate the validation set by batches.\n",
        "        for batch in tqdm(valid_loader):\n",
        "\n",
        "            # A batch consists of image data and corresponding labels.\n",
        "            imgs, labels = batch\n",
        "\n",
        "            # We don't need gradient in validation.\n",
        "            # Using torch.no_grad() accelerates the forward process.\n",
        "            with torch.no_grad():\n",
        "                logits = model(imgs.to(device))\n",
        "\n",
        "            # We can still compute the loss (but not the gradient).\n",
        "            losses.append(criterion(logits, labels.to(device)).item())\n",
        "\n",
        "            # Compute the accuracy for current batch.\n",
        "            accs.append((logits.argmax(dim=-1) == labels.to(device)).float().mean().item())\n",
        "\n",
        "    return np.mean(accs), np.mean(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h01dsY-meFVR"
      },
      "source": [
        "## **開始訓練**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rlXw2cTfeFVR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "###################################################################################\n",
        "def main(logdir):\n",
        "\n",
        "    config = DefualtConfig()\n",
        "    device = torch.device(f'cuda:{config.use_gpu_index}' if torch.cuda.is_available() else'cpu') if config.use_gpu_index != -1 else torch.device('cpu')\n",
        "\n",
        "\n",
        "    # Step 1 : prepare logging writer\n",
        "    writer = SummaryWriter(log_dir=logdir)\n",
        "\n",
        "    # Step 2 : \n",
        "    print(config.model_name)\n",
        "    # model = getattr(models, config.model_name)(config)\n",
        "    model = None\n",
        "    if config.model_name == 'STN_ViT':\n",
        "        model = STN_ViT(config)\n",
        "    elif config.model_name == 'Swin_ViT':\n",
        "        model = Swin_ViT(config)\n",
        "    elif config.model_name == 'ConvNeXt':\n",
        "        model = ConvNeXt(config)\n",
        "\n",
        "    if config.load_model:\n",
        "        model.load_state_dict(torch.load(config.model_path))\n",
        "    model.to(device)\n",
        "\n",
        "    # \n",
        "    # Metrics : FLOPs, Params\n",
        "    # resize = (224, 224) if config.model_name != 'ConvNeXt' else (384, 384)\n",
        "    resize = (config.resize, config.resize)\n",
        "    macs, params = get_model_complexity_info(model, (3, resize[0], resize[1]), as_strings=True, print_per_layer_stat=False, verbose=False)\n",
        "    print('pthflops : ')\n",
        "    print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
        "    print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
        "\n",
        "    # Step 3 : DataSets}\n",
        "    # Data Augumentation\n",
        "    transform_set = [\n",
        "        transforms.RandomResizedCrop((resize[0])),\n",
        "        # transforms.ColorJitter(brightness=0.5),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        # transforms.RandAugment()\n",
        "        # transforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET)\n",
        "    ]\n",
        "\n",
        "    transform_set = transforms.Compose([\n",
        "\n",
        "        # # Reorder transform randomly\n",
        "        transforms.RandomOrder(transform_set),\n",
        "\n",
        "        # Resize the image into a fixed shape\n",
        "        transforms.Resize(resize),\n",
        "\n",
        "        # ToTensor() should be the last one of the transforms.\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "        # \n",
        "        transforms.RandomErasing()\n",
        "    ])\n",
        "    ds = OrchidDataSet(config.trainset_path, transform_set=transform_set)\n",
        "    ds_unlabeled = None\n",
        "    if config.do_semi:\n",
        "        ds_unlabeled = OrchidDataSet(config.unlabeledset_path, transform_set=transform_set)\n",
        "\n",
        "    # Step 3\n",
        "    # Deal with imbalance dataset\n",
        "    #   For the classification task, we use cross-entropy as the measurement of performance.\n",
        "    #   Since the wafer dataset is serverly imbalance, we add class weight to make it classifier better\n",
        "    class_weights = [1 - (ds.targets.count(c))/len(ds) for c in range(config.num_classes)]\n",
        "    class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
        "    # criterion = LabelSmoothingCrossEntropy()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=1e-8)\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
        "    # optimizer = torch.optim.SGD(model.parameters(), lr=config.lr, momentum=0.9)\n",
        "\n",
        "    ema = ExponentialMovingAverage(model.parameters(), decay=0.995)\n",
        "    if config.load_model:\n",
        "        ema = ema.load_state_dict(torch.load(config.ema_path))\n",
        "\n",
        "    # scheduler_warmup is chained with schduler_steplr\n",
        "    # scheduler_steplr = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "    scheduler_steplr = CosineAnnealingLR(optimizer, T_max=config.cosine_tmax)\n",
        "    # scheduler_steplr = CosineAnnealingLR(optimizer, T_max=config.num_epochs - config.lr_warmup_epoch + 1)\n",
        "    # scheduler_steplr = ExponentialLR(optimizer, gamma=0.9)\n",
        "    # if config.lr_warmup_epoch > 0:\n",
        "    scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=config.lr_warmup_epoch, after_scheduler=scheduler_steplr)\n",
        "\n",
        "    # Step 4\n",
        "    # train_loader, valid_loader = get_loader(ds)\n",
        "    ds_train, ds_valid = get_train_valid_ds(ds)\n",
        "\n",
        "    # Step 5\n",
        "    history = {'train_acc' : [], 'train_loss' : [], 'valid_acc' : [], 'valid_loss' : []}\n",
        "    best_epoch, best_epoch_ema, best_loss, best_acc_ema, best_acc = 0, 0, 1e100, 0, 0\n",
        "    nonImprove_epochs = 0\n",
        "\n",
        "    # this zero gradient update is needed to avoid a warning message, issue #8.\n",
        "    optimizer.zero_grad()\n",
        "    optimizer.step()\n",
        "\n",
        "    #\n",
        "    # assert not(config.do_cutMix and config.do_MixUp), \"Only support one of the mix-based augmentation\"\n",
        "\n",
        "    for epoch in range(config.start_epoch, config.start_epoch + config.num_epochs):\n",
        "\n",
        "        print('=' * 150)\n",
        "\n",
        "        # \n",
        "        # if config.lr_warmup_epoch > 0:\n",
        "        scheduler_warmup.step(epoch + 1)\n",
        "\n",
        "        writer.add_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], epoch)\n",
        "        print(f'Epoch {epoch}, LR = {optimizer.param_groups[0][\"lr\"]}')\n",
        "\n",
        "        # \n",
        "        collator = torch.utils.data.dataloader.default_collate\n",
        "        # if config.do_cutMix:\n",
        "        #     collator = CutMixCollator(config.cutMix_alpha)\n",
        "        \n",
        "        train_loader = DataLoader(ds_train, batch_size=config.batch_size, shuffle=True, collate_fn=collator, num_workers=config.num_workers, pin_memory=True)\n",
        "        # if epoch == 35:\n",
        "        #     torch.save(model.state_dict(), f'{config.model_path[:-4]}_normal.pth')\n",
        "        #     torch.save(ema.state_dict(), f'{config.ema_path[:-4]}_normal.pth')\n",
        "            \n",
        "        if epoch >= config.semi_start_epoch and config.do_semi:\n",
        "            # Obtain pseudo-labels for unlabeled data using trained model.\n",
        "            print(f\"[ Train | Start pseudo labeling]\")\n",
        "            pseudo_set = get_pseudo_labels(model, ds_unlabeled)\n",
        "\n",
        "            if pseudo_set != None:\n",
        "                # Construct a new dataset and a data loader for training.\n",
        "                # This is used in semi-supervised learning only.\n",
        "                concat_dataset = ConcatDataset([ds_train, pseudo_set])\n",
        "                train_loader = DataLoader(concat_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collator, num_workers=config.num_workers, pin_memory=True)\n",
        "                \n",
        "        valid_loader = DataLoader(ds_valid, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers, pin_memory=True)\n",
        "\n",
        "        # \n",
        "        train_criterion = criterion\n",
        "        # if config.do_cutMix:\n",
        "        #     train_criterion = CutMixCriterion(reduction='mean', label_smoothing=0.1)\n",
        "\n",
        "        train_acc, train_loss = train(model, train_loader, train_criterion, optimizer, ema)\n",
        "        print(f\"[ Train | {epoch + 1:03d}/{config.num_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
        "        \n",
        "        # \n",
        "        valid_acc, valid_loss = valid(model, valid_loader, criterion, None)\n",
        "        print(f\"[ Valid | {epoch + 1:03d}/{config.num_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
        "        \n",
        "        # \n",
        "        valid_acc_ema, valid_loss_ema = valid(model, valid_loader, criterion, ema)\n",
        "        print(f\"[ Valid | {epoch + 1:03d}/{config.num_epochs:03d} ] loss = {valid_loss_ema:.5f}, acc = {valid_acc_ema:.5f} (EMA)\")\n",
        "        \n",
        "\n",
        "        # Append the training statstics into history\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['valid_acc'].append(valid_acc)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['valid_loss'].append(valid_loss)\n",
        "\n",
        "        # Tensorboard Visualization\n",
        "        writer.add_scalar(\"Train/train_acc\", train_acc, epoch)\n",
        "        writer.add_scalar(\"Valid/valid_acc\", valid_acc, epoch)\n",
        "        writer.add_scalar(\"Valid/valid_acc_ema\", valid_acc_ema, epoch)\n",
        "        writer.add_scalar(\"Train/train_loss\", train_loss, epoch)\n",
        "        writer.add_scalar(\"Valid/valid_loss\", valid_loss, epoch)\n",
        "        writer.add_scalar(\"Valid/valid_loss_ema\", valid_loss_ema, epoch)\n",
        "\n",
        "        #\n",
        "        if valid_acc > best_acc:\n",
        "            best_acc = valid_acc\n",
        "            torch.save(model.state_dict(), f'{logdir}/{config.model_path}')\n",
        "            torch.save(ema.state_dict(), f'{logdir}/{config.ema_path}')\n",
        "            get_confidence_score(model, loader=valid_loader, use_gpu_index=config.use_gpu_index, batch_size=config.batch_size, outpu_file_path=f'{logdir}/prediction-Confidence-best.csv')\n",
        "            print(f'Saving model with acc {valid_acc:.4f} and loss {valid_loss:.4f}')\n",
        "\n",
        "        # EarlyStop\n",
        "        # if the model improves, save a checkpoint at this epoch\n",
        "        if valid_acc_ema > best_acc_ema:\n",
        "            best_loss = valid_loss_ema\n",
        "            best_acc_ema = valid_acc_ema\n",
        "            best_epoch = epoch\n",
        "            torch.save(model.state_dict(), f'{logdir}/ema_{config.model_path}')\n",
        "            torch.save(ema.state_dict(), f'{logdir}/ema_{config.ema_path}')\n",
        "            get_confidence_score(model, loader=valid_loader, use_gpu_index=config.use_gpu_index, batch_size=config.batch_size, outpu_file_path=f'{logdir}/prediction-Confidence-best-ema.csv')\n",
        "            print(f'Saving model with acc {valid_acc_ema:.4f} and loss {valid_loss_ema:.4f} (EMA)')\n",
        "            nonImprove_epochs = 0\n",
        "        else:\n",
        "            nonImprove_epochs += 1\n",
        "\n",
        "        # Stop training if your model stops improving for \"config['early_stop']\" epochs.    \n",
        "        if nonImprove_epochs >= config.earlyStop_interval:\n",
        "            break\n",
        "    \n",
        "    torch.save(model.state_dict(), f'{logdir}/last_{config.model_path}')\n",
        "    torch.save(ema.state_dict(), f'{logdir}/last_{config.ema_path}')\n",
        "    print(f'Best epoch: {best_epoch} with acc {best_acc:.4f}')\n",
        "    print(f'Best epoch: {best_epoch_ema} with acc {best_acc_ema:.4f} (EMA)')\n",
        "\n",
        "    writer.flush()\n",
        "    writer.close()\n",
        "\n",
        "    # Step 6 : Explanation & Visualization\n",
        "    get_confidence_score(model, loader=valid_loader, use_gpu_index=config.use_gpu_index, batch_size=config.batch_size, outpu_file_path=f'{logdir}/last-prediction-Confidence.csv')\n",
        "\n",
        "###################################################################################\n",
        "###################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HzAXWUvueFVS"
      },
      "outputs": [],
      "source": [
        "###################################################################################\n",
        "\n",
        "def start_training(logdir):\n",
        "\n",
        "    # parser = argparse.ArgumentParser(description='AICUP - Orchid Classifier')\n",
        "\n",
        "    # # parser.add_argument('--lr', default=2e-5, type=float,\n",
        "    # #                     help='Base learning rate')\n",
        "    # # parser.add_argument('--bs', default=32, type=int, help='Batch size')\n",
        "    # # parser.add_argument('--e', default=50, type=int, help='Numbers of epoch')\n",
        "    # # parser.add_argument('--v', default=50, type=int, help='Experiment version')\n",
        "    # # parser.add_argument('--device', default=-1, type=int,\n",
        "    # #                     help='GPU index, -1 for cpu')\n",
        "    # parser.add_argument('--logdir', default='model', type=str, required=True, \n",
        "    #                             help='The folder to store the training stats of current model')\n",
        "\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    # \n",
        "    assert not os.path.isdir(os.path.join(os.getcwd(), logdir)), \"Already has a folder with the same name\"\n",
        "    os.mkdir(logdir)\n",
        "\n",
        "    # shutil.copy('./config.py', f'{logdir}/config.py')\n",
        "    main(logdir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvPP615SfI7U"
      },
      "source": [
        "## **Swin Transformer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **model_swin**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3CbxX97eFVS"
      },
      "outputs": [],
      "source": [
        "start_training('model_swin')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **model_swin_mixs_tmax101**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DefualtConfig(object):\n",
        "\n",
        "    ###################################################################\n",
        "    # Model\n",
        "    # model_name = 'STN_ViT'\n",
        "    # pretrained_model = 'google/vit-base-patch16-224-in21k'\n",
        "    \n",
        "    # model_name = 'ConvNeXt'\n",
        "    # # pretrained_model = 'facebook/convnext-base-224'\n",
        "    # pretrained_model = 'facebook/convnext-base-384'\n",
        "\n",
        "    model_name = 'Swin_ViT'\n",
        "    # pretrained_model = 'microsoft/swin-base-patch4-window7-224'\n",
        "    pretrained_model = 'microsoft/swin-base-patch4-window12-384'\n",
        "\n",
        "    # model_name = 'CVT'\n",
        "    # pretrained_model = 'microsoft/cvt-w24-384-22k'\n",
        "    \n",
        "    resize = 384\n",
        "\n",
        "    ###################################################################\n",
        "\n",
        "    model_path = 'model.pth'\n",
        "    ema_path = 'ema.pth'\n",
        "    load_model = False\n",
        "    num_classes = 219\n",
        "\n",
        "    # only one of them can be true\n",
        "    do_MixUp = True\n",
        "\n",
        "    do_cutMix = True\n",
        "    beta = 1.0\n",
        "    \n",
        "    mix_prob = 0.2\n",
        "\n",
        "    ###################################################################\n",
        "    # Training\n",
        "    start_epoch = 0\n",
        "    num_epochs = 105\n",
        "    earlyStop_interval = 600\n",
        "\n",
        "    do_semi = False\n",
        "    semi_start_epoch = 40\n",
        "\n",
        "    batch_size = 8\n",
        "    lr = 5e-5\n",
        "    lr_warmup_epoch = 5\n",
        "    cosine_tmax = 101\n",
        "\n",
        "    ###################################################################\n",
        "    # GPU Settings\n",
        "    # use_gpu = True\n",
        "    use_gpu_index = 0\n",
        "\n",
        "    ###################################################################\n",
        "    # DataLoader\n",
        "    num_workers = 6\n",
        "\n",
        "    # Dataset\n",
        "    trainset_path = './data/dataset/train'\n",
        "    unlabeledset_path = './data/dataset/unlabeled'\n",
        "    testset_path = './data/dataset/test'\n",
        "\n",
        "    train_valid_split = 0.2  # ratio of valid set\n",
        "\n",
        "    ###################################################################\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        '''\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def parse(self, kwargs):\n",
        "        '''\n",
        "        '''\n",
        "        print('User config : ')\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_training('model_swin_mixs_tmax101')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKPEv0UDf7xS"
      },
      "source": [
        "## **ConvNeXt**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **model_convnext_384_tmax101**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "0L74uiL9gAji"
      },
      "outputs": [],
      "source": [
        "class DefualtConfig(object):\n",
        "\n",
        "    ###################################################################\n",
        "    # Model\n",
        "    # model_name = 'STN_ViT'\n",
        "    # pretrained_model = 'google/vit-base-patch16-224-in21k'\n",
        "    \n",
        "    model_name = 'ConvNeXt'\n",
        "    # pretrained_model = 'facebook/convnext-base-224'\n",
        "    pretrained_model = 'facebook/convnext-base-384'\n",
        "\n",
        "    # model_name = 'Swin_ViT'\n",
        "    # pretrained_model = 'microsoft/swin-base-patch4-window7-224'\n",
        "    \n",
        "    resize = 384\n",
        "\n",
        "    ###################################################################\n",
        "\n",
        "    model_path = 'model.pth'\n",
        "    ema_path = 'ema.pth'\n",
        "    load_model = False\n",
        "    num_classes = 219\n",
        "\n",
        "    # only one of them can be true\n",
        "    do_MixUp = False\n",
        "\n",
        "    do_cutMix = True\n",
        "    beta = 1.0\n",
        "    \n",
        "    mix_prob = 0.2\n",
        "\n",
        "    ###################################################################\n",
        "    # Training\n",
        "    start_epoch = 0\n",
        "    num_epochs = 105\n",
        "    earlyStop_interval = 600\n",
        "\n",
        "    do_semi = False\n",
        "    semi_start_epoch = 40\n",
        "\n",
        "    batch_size = 8\n",
        "    lr = 5e-5\n",
        "    lr_warmup_epoch = 5\n",
        "    cosine_tmax = 101\n",
        "\n",
        "    ###################################################################\n",
        "    # GPU Settings\n",
        "    # use_gpu = True\n",
        "    use_gpu_index = 0\n",
        "\n",
        "    ###################################################################\n",
        "    # DataLoader\n",
        "    num_workers = 6\n",
        "\n",
        "    # Dataset\n",
        "    trainset_path = './data/dataset/train'\n",
        "    unlabeledset_path = './data/dataset/unlabeled'\n",
        "    testset_path = './data/dataset/test'\n",
        "\n",
        "    train_valid_split = 0.2  # ratio of valid set\n",
        "\n",
        "    ###################################################################\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        '''\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def parse(self, kwargs):\n",
        "        '''\n",
        "        '''\n",
        "        print('User config : ')\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDR2OXKqgBbZ"
      },
      "outputs": [],
      "source": [
        "start_training('model_convnext_384_tmax101')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaeLr0lKf97l"
      },
      "source": [
        "## **STN-ViT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **model_stnvit_mixs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deBkjM-zgBND"
      },
      "outputs": [],
      "source": [
        "class DefualtConfig(object):\n",
        "\n",
        "    ###################################################################\n",
        "    # Model\n",
        "    # model_name = 'STN_ViT'\n",
        "    # pretrained_model = 'google/vit-base-patch16-224-in21k'\n",
        "    \n",
        "    # model_name = 'ConvNeXt'\n",
        "    # # pretrained_model = 'facebook/convnext-base-224'\n",
        "    # pretrained_model = 'facebook/convnext-base-384'\n",
        "\n",
        "    model_name = 'Swin_ViT'\n",
        "    # pretrained_model = 'microsoft/swin-base-patch4-window7-224'\n",
        "    pretrained_model = 'microsoft/swin-base-patch4-window12-384'\n",
        "\n",
        "    # model_name = 'CVT'\n",
        "    # pretrained_model = 'microsoft/cvt-w24-384-22k'\n",
        "    \n",
        "    resize = 384\n",
        "\n",
        "    ###################################################################\n",
        "\n",
        "    model_path = 'model.pth'\n",
        "    ema_path = 'ema.pth'\n",
        "    load_model = False\n",
        "    num_classes = 219\n",
        "\n",
        "    # only one of them can be true\n",
        "    do_MixUp = True\n",
        "\n",
        "    do_cutMix = True\n",
        "    beta = 1.0\n",
        "    \n",
        "    mix_prob = 0.2\n",
        "\n",
        "    ###################################################################\n",
        "    # Training\n",
        "    start_epoch = 0\n",
        "    num_epochs = 105\n",
        "    earlyStop_interval = 600\n",
        "\n",
        "    do_semi = False\n",
        "    semi_start_epoch = 40\n",
        "\n",
        "    batch_size = 8\n",
        "    lr = 5e-5\n",
        "    lr_warmup_epoch = 5\n",
        "    cosine_tmax = 101\n",
        "\n",
        "    ###################################################################\n",
        "    # GPU Settings\n",
        "    # use_gpu = True\n",
        "    use_gpu_index = 0\n",
        "\n",
        "    ###################################################################\n",
        "    # DataLoader\n",
        "    num_workers = 6\n",
        "\n",
        "    # Dataset\n",
        "    trainset_path = './data/dataset/train'\n",
        "    unlabeledset_path = './data/dataset/unlabeled'\n",
        "    testset_path = './data/dataset/test'\n",
        "\n",
        "    train_valid_split = 0.2  # ratio of valid set\n",
        "\n",
        "    ###################################################################\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        '''\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def parse(self, kwargs):\n",
        "        '''\n",
        "        '''\n",
        "        print('User config : ')\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UozSU6pLgA-r"
      },
      "outputs": [],
      "source": [
        "start_training('model_stnvit_mixs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO4Zo7yVeFVT"
      },
      "source": [
        "# **資料預測**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "b4xQ1fZ8eFVT"
      },
      "outputs": [],
      "source": [
        "###################################################################################\n",
        "\n",
        "# config = DefualtConfig()\n",
        "# device = torch.device(f'cuda:{config.use_gpu_index}' if torch.cuda.is_available() else'cpu') if config.use_gpu_index != -1 else torch.device('cpu')\n",
        "config = None\n",
        "device = torch.device('cpu')\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def get_fileName(ds):\n",
        "\n",
        "    fileNames = []\n",
        "\n",
        "    for i in range(len(ds.imgs)):\n",
        "        fileNames.append(ds.imgs[i][0])\n",
        "\n",
        "    return fileNames\n",
        "\n",
        "\n",
        "def test(output_file_path='predictions.csv', model_name='model'):\n",
        "    '''\n",
        "    @ Params:\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Mapping\n",
        "    config = DefualtConfig()\n",
        "    ds = OrchidDataSet(config.trainset_path, transform_set=None)\n",
        "    idx_to_class = {}\n",
        "    for k in ds.class_to_idx:\n",
        "        idx_to_class[ds.class_to_idx[k]] = k\n",
        "\n",
        "    # Step 1 : Model Define & Load\n",
        "    # model = getattr(models, config.model_name)(config)\n",
        "    model = None\n",
        "    print(config.model_name)\n",
        "    if config.model_name == 'STN_ViT':\n",
        "        model = STN_ViT(config)\n",
        "    elif config.model_name == 'Swin_ViT':\n",
        "        model = Swin_ViT(config)\n",
        "    elif config.model_name == 'ConvNeXt':\n",
        "        model = ConvNeXt(config)\n",
        "\n",
        "    device = torch.device(f'cuda:{config.use_gpu_index}' if torch.cuda.is_available() else'cpu') if config.use_gpu_index != -1 else torch.device('cpu')\n",
        "    model = model.to(device)\n",
        "    if torch.cuda.is_available() is True:\n",
        "        model = model.cuda()\n",
        "    model.load_state_dict(torch.load(f'./saved/{model_name}/{config.model_path}', map_location=device))\n",
        "\n",
        "    ema = ExponentialMovingAverage(model.parameters(), decay=0.995)\n",
        "    ema.load_state_dict(torch.load(f'./saved/{model_name}/{config.ema_path}', map_location=device))\n",
        "\n",
        "    # Step 2 : DataSet & DataLoader\n",
        "    resize = resize = (config.resize, config.resize)\n",
        "    transform_set = transforms.Compose([\n",
        "\n",
        "        # Resize the image into a fixed shape\n",
        "        transforms.Resize(resize),\n",
        "\n",
        "        # ToTensor() should be the last one of the transforms.\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    ds_test = OrchidDataSet(config.testset_path, transform_set=transform_set)\n",
        "    # test_loader = DataLoader(ds_test, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)\n",
        "    test_loader = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=config.num_workers)\n",
        "\n",
        "    # Step 3 : Make prediction via trained model\n",
        "    # Make sure the model is in eval mode.\n",
        "    # Some modules like Dropout or BatchNorm affect if the model is in training mode.\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize a list to store the predictions.\n",
        "    predictions = []\n",
        "\n",
        "    with ema.average_parameters():\n",
        "\n",
        "        # Iterate the validation set by batches.\n",
        "        for batch in tqdm(test_loader):\n",
        "\n",
        "            # A batch consists of image data and corresponding labels.\n",
        "            imgs, _ = batch\n",
        "\n",
        "            # We don't need gradient in validation.\n",
        "            # Using torch.no_grad() accelerates the forward process.\n",
        "            with torch.no_grad():\n",
        "                logits = model(imgs.to(device))\n",
        "\n",
        "            predictions += logits.argmax(dim=-1)\n",
        "\n",
        "    # imgs_file_names = os.listdir(config.testset_path)\n",
        "    imgs_file_names = get_fileName(ds_test)\n",
        "\n",
        "    # Step 4 : Save predictions into the file.\n",
        "    with open(output_file_path, \"w\") as f:\n",
        "\n",
        "        # The first row must be \"Id, Category\"\n",
        "        f.write(\"filename,category\\n\")\n",
        "\n",
        "        # For the rest of the rows, each image id corresponds to a predicted class.\n",
        "        for i, pred in  enumerate(predictions):\n",
        "            ans = idx_to_class[pred.item()]\n",
        "            imgs_file_name = imgs_file_names[i].split(\"\\\\\")\n",
        "            # f.write(f\"{imgs_file_names[i][-16:]},{ans}\\n\")\n",
        "            f.write(f\"{imgs_file_name[-1]},{ans}\\n\")\n",
        "\n",
        "    # Step 5 : Rearrange\n",
        "    # Rearrange the predictions into a dataframe.\n",
        "    df_predict = pd.read_csv(output_file_path)\n",
        "    df_sample = pd.read_csv('./submission_template.csv')\n",
        "    df_arrange = pd.merge(df_sample, df_predict, how='inner', on=['filename'])\n",
        "    df_arrange = df_arrange.drop('category_sample', axis=1)\n",
        "    df_arrange.to_csv(f'rearrange_{output_file_path}', index=False)\n",
        "\n",
        "    # \n",
        "    # # Step 6 : Explanation & Visualization\n",
        "    # # get_confidence_score(model, loader=test_loader, use_gpu_index=config.use_gpu_index, batch_size=config.batch_size, outpu_file_path=f'{output_file_path[:-3]}_Confidence.csv')\n",
        "    # get_confidence_score(model, loader=test_loader, use_gpu_index=config.use_gpu_index, batch_size=BATCH_SIZE, outpu_file_path=f'{output_file_path[:-3]}_Confidence.csv')\n",
        "\n",
        "\n",
        "###################################################################################\n",
        "###################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "baTkF_3ceFVU"
      },
      "outputs": [],
      "source": [
        "def start_testing(model):\n",
        "\n",
        "    # parser = argparse.ArgumentParser(description='AICUP - Orchid Classifier')\n",
        "\n",
        "    # # parser.add_argument('--lr', default=2e-5, type=float,\n",
        "    # #                     help='Base learning rate')\n",
        "    # # parser.add_argument('--bs', default=32, type=int, help='Batch size')\n",
        "    # # parser.add_argument('--e', default=50, type=int, help='Numbers of epoch')\n",
        "    # # parser.add_argument('--v', default=50, type=int, help='Experiment version')\n",
        "    # # parser.add_argument('--device', default=-1, type=int,\n",
        "    # #                     help='GPU index, -1 for cpu')\n",
        "    # # parser.add_argument('--logdir', default='model', type=str, required=True, \n",
        "    # #                             help='The folder to store the training stats of current model')\n",
        "\n",
        "    # parser.add_argument('--model', default='model_swin', type=str, required=True,\n",
        "    #                             help='The name of the model')\n",
        "\n",
        "    # parser.add_argument('--output', default=f'predictions.csv', type=str,\n",
        "    #                             help='The file to store the predictions')\n",
        "\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    # # First, select the chosen model's config file, and replace it with the current one in the main folder\n",
        "    # # Remove the old config file\n",
        "    # if os.path.exists('config.py'):\n",
        "    #     os.remove('config.py')\n",
        "    \n",
        "    # # Then copy the new config file from the model folder, and rename it to config.py\n",
        "    # shutil.copyfile(f'./saved/{model}/config.py', 'config.py')\n",
        "\n",
        "    # # \n",
        "    # config = DefualtConfig()\n",
        "\n",
        "    # Second\n",
        "    test(output_file_path=f'prediction_{model}.csv', model_name=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **model_stnvit_mixs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DefualtConfig(object):\n",
        "\n",
        "    ###################################################################\n",
        "    # Model\n",
        "    model_name = 'STN_ViT'\n",
        "    pretrained_model = 'google/vit-base-patch16-224-in21k'\n",
        "    # pretrained_model = 'google/vit-base-patch32-384'\n",
        "    \n",
        "    # model_name = 'ConvNeXt'\n",
        "    # # pretrained_model = 'facebook/convnext-base-224'\n",
        "    # pretrained_model = 'facebook/convnext-base-384'\n",
        "\n",
        "    # model_name = 'Swin_ViT'\n",
        "    # # pretrained_model = 'microsoft/swin-base-patch4-window7-224'\n",
        "    # pretrained_model = 'microsoft/swin-base-patch4-window12-384'\n",
        "\n",
        "    # model_name = 'CVT'\n",
        "    # pretrained_model = 'microsoft/cvt-w24-384-22k'\n",
        "    \n",
        "    resize = 384\n",
        "\n",
        "    ###################################################################\n",
        "\n",
        "    model_path = 'model.pth'\n",
        "    ema_path = 'ema.pth'\n",
        "    load_model = False\n",
        "    num_classes = 219\n",
        "\n",
        "    do_MixUp = True\n",
        "\n",
        "    do_cutMix = True\n",
        "    beta = 1.0\n",
        "    \n",
        "    mix_prob = 0.2\n",
        "\n",
        "    ###################################################################\n",
        "    # Training\n",
        "    start_epoch = 0\n",
        "    num_epochs = 105\n",
        "    earlyStop_interval = 600\n",
        "\n",
        "    do_semi = False\n",
        "    semi_start_epoch = 40\n",
        "\n",
        "    batch_size = 8\n",
        "    lr = 5e-5\n",
        "    lr_warmup_epoch = 5\n",
        "    cosine_tmax = 101\n",
        "\n",
        "    ###################################################################\n",
        "    # GPU Settings\n",
        "    # use_gpu = True\n",
        "    use_gpu_index = 0\n",
        "\n",
        "    ###################################################################\n",
        "    # DataLoader\n",
        "    num_workers = 6\n",
        "\n",
        "    # Dataset\n",
        "    trainset_path = './data/dataset/train'\n",
        "    unlabeledset_path = './data/dataset/unlabeled'\n",
        "    testset_path = './data/dataset/test'\n",
        "\n",
        "    train_valid_split = 0.2  # ratio of valid set\n",
        "\n",
        "    ###################################################################\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        '''\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def parse(self, kwargs):\n",
        "        '''\n",
        "        '''\n",
        "        print('User config : ')\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqbbpdIteFVU"
      },
      "outputs": [],
      "source": [
        "start_testing('model_stnvit_mixs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **model_convnext_384_tmax101**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DefualtConfig(object):\n",
        "\n",
        "    ###################################################################\n",
        "    # Model\n",
        "    # model_name = 'STN_ViT'\n",
        "    # pretrained_model = 'google/vit-base-patch16-224-in21k'\n",
        "    \n",
        "    model_name = 'ConvNeXt'\n",
        "    # pretrained_model = 'facebook/convnext-base-224'\n",
        "    pretrained_model = 'facebook/convnext-base-384'\n",
        "\n",
        "    # model_name = 'Swin_ViT'\n",
        "    # pretrained_model = 'microsoft/swin-base-patch4-window7-224'\n",
        "    \n",
        "    resize = 384\n",
        "\n",
        "    ###################################################################\n",
        "\n",
        "    model_path = 'model.pth'\n",
        "    ema_path = 'ema.pth'\n",
        "    load_model = False\n",
        "    num_classes = 219\n",
        "\n",
        "    # only one of them can be true\n",
        "    do_MixUp = False\n",
        "\n",
        "    do_cutMix = True\n",
        "    beta = 1.0\n",
        "    \n",
        "    mix_prob = 0.2\n",
        "\n",
        "    ###################################################################\n",
        "    # Training\n",
        "    start_epoch = 0\n",
        "    num_epochs = 105\n",
        "    earlyStop_interval = 600\n",
        "\n",
        "    do_semi = False\n",
        "    semi_start_epoch = 40\n",
        "\n",
        "    batch_size = 8\n",
        "    lr = 5e-5\n",
        "    lr_warmup_epoch = 5\n",
        "    cosine_tmax = 101\n",
        "\n",
        "    ###################################################################\n",
        "    # GPU Settings\n",
        "    # use_gpu = True\n",
        "    use_gpu_index = 0\n",
        "\n",
        "    ###################################################################\n",
        "    # DataLoader\n",
        "    num_workers = 6\n",
        "\n",
        "    # Dataset\n",
        "    trainset_path = './data/dataset/train'\n",
        "    unlabeledset_path = './data/dataset/unlabeled'\n",
        "    testset_path = './data/dataset/test'\n",
        "\n",
        "    train_valid_split = 0.2  # ratio of valid set\n",
        "\n",
        "    ###################################################################\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        '''\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def parse(self, kwargs):\n",
        "        '''\n",
        "        '''\n",
        "        print('User config : ')\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_testing('model_convnext_384_tmax101')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **model_swin_mixs_tmax101**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DefualtConfig(object):\n",
        "\n",
        "    ###################################################################\n",
        "    # Model\n",
        "    # model_name = 'STN_ViT'\n",
        "    # pretrained_model = 'google/vit-base-patch16-224-in21k'\n",
        "    \n",
        "    # model_name = 'ConvNeXt'\n",
        "    # # pretrained_model = 'facebook/convnext-base-224'\n",
        "    # pretrained_model = 'facebook/convnext-base-384'\n",
        "\n",
        "    model_name = 'Swin_ViT'\n",
        "    # pretrained_model = 'microsoft/swin-base-patch4-window7-224'\n",
        "    pretrained_model = 'microsoft/swin-base-patch4-window12-384'\n",
        "\n",
        "    # \n",
        "    resize = 384\n",
        "\n",
        "    ###################################################################\n",
        "\n",
        "    model_path = 'model.pth'\n",
        "    ema_path = 'ema.pth'\n",
        "    load_model = False\n",
        "    num_classes = 219\n",
        "    \n",
        "    ###################################################################\n",
        "    # Mix-based Augmentation\n",
        "    ## Mixup\n",
        "    do_MixUp = True\n",
        "\n",
        "    ## CutMix\n",
        "    do_cutMix = True\n",
        "    beta = 1.0\n",
        "    \n",
        "    # the probability to use mix-based augmentation\n",
        "    mix_prob = 0.2\n",
        "\n",
        "    ###################################################################\n",
        "    # Training\n",
        "    ## Epochs\n",
        "    start_epoch = 0\n",
        "    num_epochs = 105            # Total epoch\n",
        "    earlyStop_interval = 600    # \n",
        "    batch_size = 8\n",
        "    lr = 5e-5\n",
        "    lr_warmup_epoch = 5         # warmup epoch\n",
        "    cosine_tmax = 101           # The Tmax for cosine annealing learning rate scheduler\n",
        "    \n",
        "    # Do semi-supervised training (if there is additional data for training)\n",
        "    do_semi = False\n",
        "    semi_start_epoch = 40\n",
        "\n",
        "    ###################################################################\n",
        "    # GPU Settings\n",
        "    ## The index of GPU to use\n",
        "    use_gpu_index = 0\n",
        "\n",
        "    ###################################################################\n",
        "    # Data\n",
        "    ## DataLoader\n",
        "    num_workers = 6\n",
        "\n",
        "    ## Dataset Location\n",
        "    trainset_path = './data/dataset/train'\n",
        "    unlabeledset_path = './data/dataset/unlabeled'\n",
        "    testset_path = './data/dataset/test'\n",
        "\n",
        "    ## Train/ valid split ratio\n",
        "    train_valid_split = 0.2  # ratio of valid set\n",
        "\n",
        "    ###################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_testing('model_swin_mixs_tmax101')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **model_swin**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DefualtConfig(object):\n",
        "\n",
        "    ###################################################################\n",
        "    # Model\n",
        "    # model_name = 'STN_ViT'\n",
        "    # pretrained_model = 'google/vit-base-patch16-224-in21k'\n",
        "    \n",
        "    # model_name = 'ConvNeXt'\n",
        "    # pretrained_model = 'facebook/convnext-base-224'\n",
        "\n",
        "    model_name = 'Swin_ViT'\n",
        "    pretrained_model = 'microsoft/swin-base-patch4-window7-224'\n",
        "    \n",
        "    resize = 224\n",
        "\n",
        "    ###################################################################\n",
        "\n",
        "    model_path = 'model.pth'\n",
        "    ema_path = 'ema.pth'\n",
        "    load_model = False\n",
        "    num_classes = 219\n",
        "\n",
        "    # only one of them can be true\n",
        "    do_MixUp = False\n",
        "\n",
        "    do_cutMix = True\n",
        "    beta = 1.0\n",
        "    \n",
        "    mix_prob = 0.2\n",
        "\n",
        "    ###################################################################\n",
        "    # Training\n",
        "    start_epoch = 0\n",
        "    num_epochs = 105\n",
        "    earlyStop_interval = 600\n",
        "\n",
        "    do_semi = False\n",
        "    semi_start_epoch = 40\n",
        "\n",
        "    batch_size = 16\n",
        "    lr = 5e-5\n",
        "    lr_warmup_epoch = 5\n",
        "    cosine_tmax = 20\n",
        "\n",
        "    ###################################################################\n",
        "    # GPU Settings\n",
        "    # use_gpu = True\n",
        "    use_gpu_index = 0\n",
        "\n",
        "    ###################################################################\n",
        "    # DataLoader\n",
        "    num_workers = 6\n",
        "\n",
        "    # Dataset\n",
        "    trainset_path = './data/dataset/train'\n",
        "    unlabeledset_path = './data/dataset/unlabeled'\n",
        "    testset_path = './data/dataset/test'\n",
        "\n",
        "    train_valid_split = 0.2  # ratio of valid set\n",
        "\n",
        "    ###################################################################\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        '''\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def parse(self, kwargs):\n",
        "        '''\n",
        "        '''\n",
        "        print('User config : ')\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_testing('model_swin')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Ensemble 模型集成**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 81710/81710 [00:24<00:00, 3386.85it/s]\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# \n",
        "# register_models = ['model_stnvit_mixs', 'model_swin_mixs_tmax101', 'model_convnext_384_tmax101']\n",
        "\n",
        "# # Register confidence score dataframe from register models\n",
        "# confidence_scores = []\n",
        "# for model_name in register_models:\n",
        "#     # Read condifence score table for each model\n",
        "#     path = f'./saved/{model_name}/prediction-Confidence-best.csv'\n",
        "#     confidence_scores.append(pd.read_csv(path))\n",
        "\n",
        "register_models = ['model_stnvit_mixs', 'model_swin', 'model_swin_mixs_tmax101', 'model_convnext_384_tmax101']\n",
        "\n",
        "# Register confidence score dataframe from register models\n",
        "confidence_scores = []\n",
        "for model_name in register_models:\n",
        "    # Read condifence score table for each model\n",
        "    path = f'./rearrange_prediction_{model_name}.csv'\n",
        "    confidence_scores.append(pd.read_csv(path))\n",
        "\n",
        "'''\n",
        "validset_index, topN, Ground_truth, 1, prob1, 2, prob2, 3, prob3, 4, prob4, 5, prob5\n",
        "0, 5, 0, 0, 0.7134590148925781, 49, 0.01988092251121998, 178, 0.009589494206011295, 200, 0.00530626904219389, 174, 0.005254499148577452\n",
        "'''\n",
        "\n",
        "df_sample = pd.read_csv('./submission_template.csv')\n",
        "\n",
        "#\n",
        "with open('./ensemble_predict.csv', 'w') as f:\n",
        "    # \n",
        "\n",
        "    n_correct = 0\n",
        "\n",
        "    N = len(confidence_scores[0])\n",
        "    for idx in tqdm(range(N)):   \n",
        "        # the ensemble result from each model\n",
        "        # the format is [label1 : total confidence score, label2 : total confidence score, ...]\n",
        "        ensemble = {}\n",
        "\n",
        "        priority = [2, 1, 3, 0]\n",
        "\n",
        "        # Add top 5 confidence score of each predicted label into ensemble\n",
        "        for confidence in confidence_scores:\n",
        "            if confidence.iloc[idx, 1] in ensemble:\n",
        "                ensemble[confidence.iloc[idx, 1]] += 1\n",
        "            else:\n",
        "                ensemble[confidence.iloc[idx, 1]] = 1\n",
        "        \n",
        "        # Sort the ensemble result\n",
        "        ensemble = sorted(ensemble.items(), key=lambda x: x[1], reverse=True)\n",
        "        # print(ensemble)\n",
        "        # break\n",
        "\n",
        "        ensemble_label = ensemble[0][0]\n",
        "\n",
        "        if len(ensemble) >= 2 and ensemble[0][1] == ensemble[1][1]:\n",
        "            for i in priority:\n",
        "                if confidence_scores[i].iloc[idx, 1] == ensemble[0][0]:\n",
        "                    ensemble_label = ensemble[0][0]\n",
        "                else:\n",
        "                    ensemble_label = ensemble[1][0]\n",
        "\n",
        "        # And then write the ensemble result into csv file\n",
        "        # f.write(f'{idx},{ensemble_label}\\n')\n",
        "        df_sample.iloc[idx, 1] = ensemble_label\n",
        "\n",
        "df_sample.to_csv('./ensemble_predict.csv', index=False)\n",
        "\n",
        "# df_predict = pd.read_csv('ensemble_predict.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.13 ('cvnets')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ac5ecde547dcadab583014e4b78d209655f55258f439d2a4a86e983d32bb8e24"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
